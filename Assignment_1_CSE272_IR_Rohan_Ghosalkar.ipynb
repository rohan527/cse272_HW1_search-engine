{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### HW 1: Search Engine"
      ],
      "metadata": {
        "id": "dOb8vufHgCCz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsgkAyuzymKM",
        "outputId": "d6b4c0d9-3f2a-4122-b68f-3506bb669775"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#I used the input files that I uploaded to my drive.\n",
        "#If you are using the colab notebook, please make sure to change the location of the files.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We import all the necessary libraries \n",
        "import re\n",
        "import math\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-aandRd5nga",
        "outputId": "20f1ec7f-daaa-4ec8-a9d1-94bf96cc455a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We will also need to install Whoosh (similar to Lucene, but in Python instead of Java)\n",
        "!pip install whoosh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuN5bjDAQOTE",
        "outputId": "91eb7af8-5387-4735-81f0-e7ee138c1631"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting whoosh\n",
            "  Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.8/468.8 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: whoosh\n",
            "Successfully installed whoosh-2.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import all the necessary modules from the Whoosh library\n",
        "from whoosh.index import create_in\n",
        "import whoosh.index as index\n",
        "from whoosh.fields import *\n",
        "from whoosh.qparser import QueryParser\n",
        "from whoosh.filedb.filestore import FileStorage\n",
        "from whoosh.filedb.filestore import RamStorage\n",
        "from whoosh.analysis import StemmingAnalyzer\n",
        "from whoosh.writing import AsyncWriter\n",
        "from whoosh.qparser import MultifieldParser\n",
        "from whoosh import qparser\n",
        "import whoosh.scoring as scoring"
      ],
      "metadata": {
        "id": "OmbjzhnzQHl8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Parsing\n",
        "\n",
        "In summary, this function reads a file containing a collection of documents in a specific format and yields each document one at a time as a dictionary with keys representing different fields (e.g., title, abstract, author) and values representing the content of those fields for the current document."
      ],
      "metadata": {
        "id": "SK_gAL7K515w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This function parses the input data that and returns it in the form of a dictionary\n",
        "def read_documents():\n",
        "    curr_doc = None\n",
        "    curr_col = None\n",
        "    # Open the file for reading\n",
        "    with open('/content/drive/MyDrive/23 - Spring Quarter/CSE 272 - IR/HW 1/ohsumed.88-91', 'r+') as lines:\n",
        "        # Loop through each line in the file\n",
        "        for line in lines:\n",
        "            # Remove the newline character at the end of the line\n",
        "            line = line[:-1]\n",
        "            # Check if the line indicates the start of a new document\n",
        "            if line.startswith(\".I\"):\n",
        "                if curr_doc is not None:\n",
        "                    # Return the current curr_doc dictionary and start for the new document\n",
        "                    yield curr_doc\n",
        "                curr_doc = dict()\n",
        "            # We check for the UID\n",
        "            elif line.startswith('.U'):\n",
        "                curr_col = \"docid\"\n",
        "            # We check for the source\n",
        "            elif line.startswith('.S'):\n",
        "                curr_col = \"source\"\n",
        "            # We check for the mesh terms\n",
        "            elif line.startswith('.M'):\n",
        "                curr_col = \"mesh_terms\"\n",
        "            # We check for the title\n",
        "            elif line.startswith('.T'):\n",
        "                curr_col = \"title\"\n",
        "            # We check for publication\n",
        "            elif line.startswith('.P'):\n",
        "                curr_col = \"publication\"\n",
        "            # We check for abstract\n",
        "            elif line.startswith('.W'):\n",
        "                curr_col = \"abstract\"\n",
        "            # We check for author\n",
        "            elif line.startswith('.A'):\n",
        "                curr_col = \"author\"\n",
        "            else:\n",
        "                curr_doc[curr_col] = line\n",
        "    # Yield the final document\n",
        "    yield curr_doc\n",
        "    return"
      ],
      "metadata": {
        "id": "8f_Wm0Gu54Ie"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query Parsing\n",
        "\n",
        "In summary, this code reads a file containing a set of queries in a specific format and parses each query, creating a dictionary object for each one. Each dictionary object contains the query number, query title, and query description. These dictionaries are then stored in a list of queries for later use."
      ],
      "metadata": {
        "id": "FVOS_ebr576J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "queries = list()\n",
        "# Open the file containing the queries for reading\n",
        "with open(\"/content/drive/MyDrive/23 - Spring Quarter/CSE 272 - IR/HW 1/query.ohsu.1-63\", \"r+\") as lines:\n",
        "    current_query = None\n",
        "    # Loop through each line in the file\n",
        "    for line in lines:\n",
        "        # Remove the newline character at the end of the line\n",
        "        line = line[:-1]\n",
        "        # Check if the line indicates the start of a new query\n",
        "        if '<top>' in line:\n",
        "            current_query = dict()\n",
        "        # Check if the line indicates the end of a query\n",
        "        elif '</top>' in line:\n",
        "            # Add the completed query to the list of queries and start a new query\n",
        "            queries.append(current_query)\n",
        "            current_query = dict()\n",
        "        # Check if the line contains the query number\n",
        "        elif '<num>' in line:\n",
        "            current_query[\"num\"] = line.split(':')[1].strip()\n",
        "        # Check if the line contains the query title\n",
        "        elif '<title>' in line:\n",
        "            current_query[\"title\"] = line.split('>')[1].strip()\n",
        "        # Check if the line contains the query description\n",
        "        elif (not '<desc>' in line and len(line) > 2):\n",
        "            current_query[\"description\"] = line"
      ],
      "metadata": {
        "id": "ya3b-oaX57U7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indexing\n",
        "\n",
        "This function creates an index using the Whoosh library. It defines a schema with several fields (title, abstract, mesh_terms, publication, author, docid), each with specific properties such as the type of indexing or whether they are stored in the index.\n",
        "\n",
        "The function then creates an index using the schema, and initializes an AsyncWriter object to add documents to the index. It loops through all the documents in the ohsumed.88-91 file, reading them using the read_documents() function, and adds them to the index using the AsyncWriter.\n",
        "\n",
        "Each document is added to the index with only the filtered_fields specified in the schema using add_document() function. The writer is then committed after every 50000th document is added to the index.\n",
        "\n",
        "Finally, the storage is closed and the function returns the created index."
      ],
      "metadata": {
        "id": "WKPWd-6K6oPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_index():\n",
        "    # Define the fields to be indexed and stored\n",
        "    filtered_fields = [\"title\", \"abstract\", \"mesh_terms\", \"publication\", \"author\", \"docid\"]\n",
        "    schema = Schema(title=TEXT(stored=False, phrase=False), \n",
        "    abstract=TEXT(stored=False, phrase=True, analyzer=StemmingAnalyzer(stoplist=stopwords.words('english'))), mesh_terms=KEYWORD(stored=False), \n",
        "    publication = KEYWORD(stored=False), \n",
        "    author = KEYWORD(stored=False), \n",
        "    docid = ID(stored=True))\n",
        "\n",
        "    # Create the index storage\n",
        "    storage = FileStorage(\"indexed\").create()\n",
        "\n",
        "    # Create the index\n",
        "    ix = storage.create_index(schema)\n",
        "    \n",
        "    # Create an async writer to add documents to the index\n",
        "    writer = AsyncWriter(ix, delay=0.2, writerargs={'limitmb':1024, 'procs':8, 'segment':True})\n",
        "\n",
        "    # Iterate over the documents and add them to the index\n",
        "    for i, d in enumerate(read_documents()):\n",
        "        writer.add_document(**{k:v for k,v in d.items() if k in filtered_fields})\n",
        "\n",
        "    # Commit the changes to the index and close the storage\n",
        "    writer.commit()\n",
        "    storage.close()\n",
        "    print((\"Indexing done\"))\n",
        "\n",
        "    # Return the index\n",
        "    return ix"
      ],
      "metadata": {
        "id": "oUg2ZzkW6ocY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boolean Algorithm\n",
        "\n",
        "Boolean search or inverted index search helps us identify the document's relevancy. After we have indexed all of our documents, we do an inverted index search based on either AND/OR/NOT. The output is a match based on the search terms and how much of a match it is with the corpus or input set. "
      ],
      "metadata": {
        "id": "4jO9lCqM6ojs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def boolean(index_dir, queries, outputfile='boolean.txt', run='boolean'):\n",
        "    # Open the index directory\n",
        "    ix = index.open_dir(index_dir)\n",
        "\n",
        "    # Create a query parser for multiple fields with OrGroup\n",
        "    qp = MultifieldParser([\"abstract\",\"title\"], schema=ix.schema, group=qparser.OrGroup)\n",
        "\n",
        "    # Print the index\n",
        "    print(ix)\n",
        "\n",
        "    with ix.searcher() as s:\n",
        "\n",
        "        with open(outputfile,'w+') as output:\n",
        "\n",
        "            for i, q in enumerate(queries):\n",
        "\n",
        "                # Parse the query using the query parser\n",
        "                query = qp.parse(q['description'])\n",
        "\n",
        "                # Search the index with the parsed query and retrieve top 50 results\n",
        "                results = s.search(query, limit=50)\n",
        "\n",
        "                # Iterate over each result\n",
        "                for rank, result in enumerate(results):\n",
        "\n",
        "                    # Write the result to the output file in the required format\n",
        "                    output.write('{0} Q0 {1} {2} {3} {4}\\n'.format(q[\"num\"], result['docid'], result.rank, result.score, run))"
      ],
      "metadata": {
        "id": "D08_W6WB6oo4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Term Frequency (tf) Algorithm\n",
        "\n",
        "Term frequency (TF) search is a technique used by search engines to rank search results based on the frequency of the search terms in each document. The idea behind TF search is that documents containing a higher frequency of search terms will likely be more relevant to the user's query."
      ],
      "metadata": {
        "id": "m2AemBGf6owd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tf(index_dir, queries, outputfile='tf.txt', run='tf'):\n",
        "    # Open the index directory\n",
        "    ix = index.open_dir(index_dir)\n",
        "\n",
        "    # Create a query parser for multiple fields with OrGroup\n",
        "    qp = MultifieldParser([\"abstract\",\"title\"], schema=ix.schema, group=qparser.OrGroup)\n",
        "\n",
        "    with ix.searcher() as s:\n",
        "\n",
        "        with open(outputfile,'w+') as output:\n",
        "          \n",
        "            for i, q in enumerate(queries):\n",
        "\n",
        "                # Parse the query using the query parser\n",
        "                query = qp.parse(q['description'])\n",
        "\n",
        "                # Search the index with the parsed query and retrieve top 50 results\n",
        "                results = s.search(query, limit=50)\n",
        "\n",
        "                # Iterate over each result\n",
        "                for rank, result in enumerate(results):\n",
        "\n",
        "                    scores = np.log(1 + result.score) \n",
        "\n",
        "                    # Write the result to the output file in the required format\n",
        "                    output.write('{0} Q0 {1} {2} {3} {4}\\n'.format(q[\"num\"], result['docid'], result.rank, scores, run))"
      ],
      "metadata": {
        "id": "lT7aBZgD6o03"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Term Frequency - Inverse Document Frequency (td-idf) Algorithm\n",
        "\n",
        "Term frequency-inverse document frequency (TF-IDF) search is a technique used by search engines to rank search results based on the relevance of the search terms to the documents in a collection. TF-IDF search considers the frequency of the search terms in each document and the rarity of the search terms across the entire collection."
      ],
      "metadata": {
        "id": "oEdDb6db63jm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_idf(index_dir, queries, outputfile='evaluation.txt', run='tf_idf'):\n",
        "    # Open the index directory\n",
        "    ix = index.open_dir(index_dir)\n",
        "\n",
        "    # Create a query parser for multiple fields with OrGroup\n",
        "    qp = MultifieldParser([\"abstract\",\"title\"], schema=ix.schema, group=qparser.OrGroup)\n",
        "    print(ix)\n",
        "\n",
        "\n",
        "    with ix.searcher(weighting=scoring.TF_IDF()) as s:\n",
        "\n",
        "        with open(outputfile,'w+') as output:\n",
        "\n",
        "            for i, q in enumerate(queries):\n",
        "\n",
        "                # Parse the query using the query parser\n",
        "                query = qp.parse(q['description'])\n",
        "\n",
        "                # Search the index with the parsed query and retrieve top 50 results\n",
        "                results = s.search(query, limit=50)\n",
        "\n",
        "                # Iterate over each result\n",
        "                for rank, result in enumerate(results):\n",
        "\n",
        "                    # Write the result to the output file in the required format\n",
        "                    output.write('{0} Q0 {1} {2} {3} {4}\\n'.format(q[\"num\"], result['docid'], result.rank, result.score, run))"
      ],
      "metadata": {
        "id": "5jdko_1k63oy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom Algorithm\n",
        "\n",
        "The score is defined as the sum of the product of the weight and the number of occurrences of each query term in the title and abstract fields, plus a constant factor. Weights are assigned based on the field: 5 for the title field, 2 for the abstract field. If a query term occurs in both fields of a document, the maximum weight is used.\n",
        "\n",
        "In the  code, we define a custom scoring function custom_score_fn that calculates the score for each document based on the number of occurrences of each query term in the title and abstract fields. We assign weights based on the field and add a constant factor to the final score. We then use this function in the with ix.searcher(weighting=custom_score_fn) as s: line to apply it during search."
      ],
      "metadata": {
        "id": "ciIGbYce63tw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_score_fn(searcher, fieldname, text, matcher):\n",
        "    score = 0\n",
        "    if fieldname == 'title':\n",
        "        weight = 5\n",
        "    elif fieldname == 'abstract':\n",
        "        weight = 2\n",
        "    else:\n",
        "        weight = 1\n",
        "        \n",
        "    for t in text:\n",
        "        df = searcher.doc_frequency(fieldname, t)\n",
        "        tf = matcher.value_as(\"frequency\")\n",
        "        if df > 0:\n",
        "            score += weight * tf * (1 + math.log(searcher.doc_count() / df))\n",
        "    score += 0.5  # constant factor\n",
        "    return score\n",
        "\n",
        "\n",
        "def custom(index_dir, queries, outputfile='custom.txt', run='custom'):\n",
        "    ix = index.open_dir(index_dir)\n",
        "    qp = MultifieldParser([\"abstract\",\"title\"], schema=ix.schema, group=qparser.OrGroup)\n",
        "    with ix.searcher(weighting=custom_score_fn) as s:\n",
        "        with open(outputfile,'w+') as output:\n",
        "            for i, q in enumerate(queries):\n",
        "                query = qp.parse(q['description'])\n",
        "                results = s.search(query, limit=50)\n",
        "                for rank, result in enumerate(results):\n",
        "                    output.write('{0} Q0 {1} {2} {3} {4}\\n'.format(q[\"num\"], result['docid'], result.rank, result.score, run))\n"
      ],
      "metadata": {
        "id": "MkBKkixL7CYU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating log files\n",
        "\n",
        "\n",
        "Here we try to create the log files with the top 50 documents that are retrived from the search algorithms along with the score for each of the documents. \n",
        "\n",
        "\n",
        "(You can uncomment the create_index() if you want to do the indexing again or else you can also simply add the folder \"indexed\" with the python file and run the code)"
      ],
      "metadata": {
        "id": "LlujCq6j7DQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create_index()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcNBRQfdP3hA",
        "outputId": "6f1b398f-4fbb-422e-8880-8ae7a6ace751"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexing done\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FileIndex(FileStorage('indexed'), 'MAIN')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  tf_idf(\"indexed\", queries, outputfile=\"tf_idf.txt\", run='TF_idf')\n",
        "  boolean(\"indexed\", queries, outputfile=\"boolean.txt\", run='boolean')\n",
        "  tf(\"indexed\", queries, outputfile=\"tf.txt\", run='TF')\n",
        "  custom(\"indexed\", queries, outputfile=\"custom.txt\", run='custom')"
      ],
      "metadata": {
        "id": "zW2vg6uc6b4P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}